{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student: Steven Hooker\n",
    "\n",
    "Project: Navigation - Banana Collector\n",
    "\n",
    "Course: Deep Reinforcement Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of this project is to train an agent to solve the “Banana Collector” environment. Within this environment the agent needs to navigate (and collect bananas!) in a large, square world.\n",
    "\n",
    "Following the given description of the challenge. \n",
    "\n",
    "A reward of +1 is provided for collecting a yellow banana, and a reward of -1 is provided for collecting a blue banana. Thus, the goal of your agent is to collect as many yellow bananas as possible while avoiding blue bananas.\n",
    "The state space has 37 dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction. Given this information, the agent has to learn how to best select actions. Four discrete actions are available, corresponding to:\n",
    "\n",
    "* 0 - move forward.\n",
    "* 1 - move backward.\n",
    "* 2 - turn left.\n",
    "* 3 - turn right.\n",
    "\n",
    "The task is episodic, and in order to solve the environment, your agent must get an average score of +13 over 100 consecutive episodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model: Deep Q Learning\n",
    "\n",
    "The challenge of reinforcement learning is to choose the best action given a specific state. \n",
    "In the Deep Q Learning approach we try to estimate the state action value function with neural networks. This function estimates the value for any possible action given a specific state and hence it provides the information which action to choose to maximize the expected future rewards (value). \n",
    "\n",
    "To solve this challenge an agent plays this episodic game and chooses action randomly at first, as the agent has no knowledge about the environment. \n",
    "Over time the agent will get feedback (positive and negative rewards) by collecting bananas and can connect the previous state, his chosen agent with the positiv and negativ reward and hence learn from it.\n",
    "\n",
    "The Model itself is a fully connected neural network with 2 hidden layers, 64 neurons each. \n",
    "The input layer has 37 neurons (like the state space) and the output layer has 4 neurons like the action space. \n",
    "\n",
    "Over hundreds of episodes it ‘experiences’ lots of different possible states and gets feedback based on the chosen action.\n",
    "\n",
    "Following the average score each 100 episodes.\n",
    "```python\n",
    "Episode 100\tAverage Score: 0.21\n",
    "Episode 200\tAverage Score: 1.86\n",
    "Episode 300\tAverage Score: 3.91\n",
    "Episode 400\tAverage Score: 5.26\n",
    "Episode 500\tAverage Score: 7.09\n",
    "Episode 600\tAverage Score: 8.01\n",
    "Episode 700\tAverage Score: 9.72\n",
    "Episode 800\tAverage Score: 10.84\n",
    "Episode 900\tAverage Score: 11.91\n",
    "Episode 1000\tAverage Score: 13.63\n",
    "Episode 1100\tAverage Score: 14.28\n",
    "Episode 1200\tAverage Score: 14.23\n",
    "Episode 1300\tAverage Score: 13.83\n",
    "Episode 1395\tAverage Score: 15.02\n",
    "Environment solved in 1295 episodes!\tAverage Score: 15.02\n",
    "```\n",
    "\n",
    "As it can be seen the challenge to get an average score of over 13 in 100 consecutive runs was achieved after 1000 episodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Following hyperparameters where used to solve this challenge.\n",
    "```python\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "```\n",
    "\n",
    "During the project different hyperparameters where tested. \n",
    "For example:\n",
    "Increasing TAU lead to a more volatile learning experience, sometimes from 100 episodes to next 100 episodes the average score increased or dropped by 50% even in the later stages. With the current TAU the average score increased slower but more consistently. \n",
    "\n",
    "The code and instructions on how start, use the trained agent and train the agent from scratch can be founds here. \n",
    "https://github.com/Python35/drl_p1_navigation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhancements\n",
    "\n",
    "The model could be enhancement by implementing prioritized experience replay, whereas the samples to which to learn from are not chosen randomly but by weights corresponding to the error between predicted and observed value. \n",
    "The thought here is if there is no gap and the agent was already predicting the state action value function for this state action pair correctly it has nothing to learn from it. On the other hand if there was a huge gap between predicted and observed value the agent should learn from it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
